{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa25ad98",
   "metadata": {},
   "source": [
    "# Coding Assignment 3: Build a Tiny Transformer\n",
    "\n",
    "**Learning goals**\n",
    "- Add **positional information** with **sinusoidal positional encodings**.\n",
    "- Understand and implement **self-attention** \n",
    "- Implement **multi-head self-attention** by splitting/combining heads.\n",
    "- Train or sample from a **tiny character-level LM** on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "931ea208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:29:56.760949Z",
     "start_time": "2025-11-07T00:29:54.318905Z"
    }
   },
   "source": [
    "# Minimal setup — CPU only, deterministic runs\n",
    "import math\n",
    "import random\n",
    "import textwrap\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 476\n",
    "random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(\"PyTorch:\", torch.__version__, \"| Device:\", DEVICE)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastianbejaoui/Desktop/Dev/Machine learning/transformer/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.2.2 | Device: cpu\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "0e24a459",
   "metadata": {},
   "source": [
    "## Dataset & Character Vocabulary\n",
    "\n",
    "In this section, we load a tiny text corpus (`shakespeare.txt`) and build a **character-level** vocabulary so our model can learn to predict the next **character**."
   ]
  },
  {
   "cell_type": "code",
   "id": "a0a47bb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:30:00.460530Z",
     "start_time": "2025-11-07T00:30:00.283496Z"
    }
   },
   "source": [
    "with open(\"shakespeare.txt\", \"r\") as f:\n",
    "    NANO_SHAKESPEARE = f.read()\n",
    "\n",
    "# --- Build a tiny char-level vocab ---\n",
    "chars = sorted(list(set(NANO_SHAKESPEARE)))\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "print(\"Unique chars:\", vocab_size)\n",
    "\n",
    "def encode(s: str):\n",
    "    return torch.tensor([stoi[c] for c in s], dtype=torch.long)\n",
    "\n",
    "def decode(ids: torch.Tensor):\n",
    "    return ''.join(itos[int(i)] for i in ids)\n",
    "\n",
    "# train/val split (character-level)\n",
    "data = encode(NANO_SHAKESPEARE)\n",
    "split = int(0.9 * len(data))\n",
    "train_ids, val_ids = data[:split], data[split:]\n",
    "len(train_ids), len(val_ids)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique chars: 65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1003853, 111540)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "9b2bd8af",
   "metadata": {},
   "source": [
    "## Background (5-minute primer)\n",
    "\n",
    "**Self-attention (one layer).** For each token, we compute a weighted sum of other token vectors. The weights come from a **similarity** between a **query** (Q) and **keys** (K):  \n",
    "`attention(Q,K,V) = softmax(QKᵀ / √dₖ) V`  \n",
    "Scaling by √dₖ keeps the dot products in a numerically friendly range.  \n",
    "We use **values** (V) as the information that actually flows forward.\n",
    "\n",
    "**Multi-head self-attention (MHA).** Instead of one attention, we project the inputs into multiple Q/K/V sets (“heads”), attend in parallel, then **concatenate** and project back. Each head can focus on different patterns.\n",
    "\n",
    "**Why positional encodings?** Attention alone doesn’t know token order. We **add** a position vector to token embeddings so the model can represent order. In this assignment, you’ll build the **sinusoidal** version.\n",
    "\n",
    "**Transformer LM and Transformer block.** We build a transformer LM that consists of many transformer blocks. One transformer block has two main components with residual connections. \n",
    "1) LayerNorm → MHA → residual add  \n",
    "2) LayerNorm → Feed-forward (GELU) → residual add\n",
    "\n",
    "### LayerNorm \n",
    "\n",
    "LayerNorm is a normalization technique that stabilizes training. We have implemented it for you. Here's an introduction if you are interested:\n",
    "- **What it does:** For each token’s feature vector (the last dimension), LayerNorm makes the values have roughly **mean 0** and **variance 1**, then applies a learned **scale (γ)** and **shift (β)**.  \n",
    "- **Why it helps:** Keeps activations in a stable range so **training is smoother**, gradients flow better, and deep Transformer stacks don’t blow up or stall.  \n",
    "- **How it differs from BatchNorm:** **BatchNorm** normalizes *across the batch* (depends on batch size/order). **LayerNorm** normalizes *within each token* (independent of batch), so it works well with **small/variable batches** and **variable sequence lengths**.  \n",
    "- **Where it goes (pre-norm):** Common today is **LayerNorm → sublayer (MHA/FFN) → residual add**. Putting LN first makes optimization more robust.  \n",
    "- **Mental model:** Think of it as “standardizing” each token’s features so every sublayer sees inputs with comparable scale, making the network easier to train.\n",
    "\n",
    "### Causal Masks\n",
    "\n",
    "For **autoregressive** language modeling (next-token prediction), training must mimic generation: the model should never use future tokens to predict the current one. We enforce this by applying a causal mask to attention scores so that, at position i, forbidden positions (positions > i) are replaced with a **large negative** number (≈ −1e9). After softmax, those entries become (near) **0**. We provide a helper method `make_causal_mask` that return a boolean mask to indicate forbidden positions. Later in this assignment you will use it to replace the forbidden positions with a large negative number (more details in Task B)."
   ]
  },
  {
   "cell_type": "code",
   "id": "44906608",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:30:10.028534Z",
     "start_time": "2025-11-07T00:30:10.021342Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    d_model: int = 128\n",
    "    n_heads: int = 4\n",
    "    n_layers: int = 2\n",
    "    ff_mult: int = 4\n",
    "    max_seq_len: int = 128\n",
    "    vocab_size: int = vocab_size\n",
    "    pe_type: str = \"sinusoidal\"  # \"none\" | \"sinusoidal\" | \"learned\"\n",
    "\n",
    "def make_causal_mask(seq_len: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns a boolean mask of shape [1, 1, seq_len, seq_len]\n",
    "    where True means \"allowed\" and False means \"masked out\".\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))\n",
    "    return mask.unsqueeze(0).unsqueeze(0)  # [1,1,S,S]\n",
    "\n",
    "def batched_random_chunks(ids: torch.Tensor, block_size: int, batch_size: int):\n",
    "    \"\"\"\n",
    "    Simple CPU-friendly batcher for quick tests/training.\n",
    "    \"\"\"\n",
    "    ix = torch.randint(0, len(ids) - block_size - 1, (batch_size,))\n",
    "    x = torch.stack([ids[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([ids[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(DEVICE), y.to(DEVICE)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "4a6c0c4c",
   "metadata": {},
   "source": [
    "## Task A — Sinusoidal Positional Encoding\n",
    "\n",
    "### What you’re building\n",
    "You will return a tensor `pe` of shape `[max_len, d_model]`. For each position `pos = 0..max_len-1` and each pair of feature columns `(2i, 2i+1)`:\n",
    "\n",
    "- `pe[pos, 2*i]   = sin( pos * ω_i )`\n",
    "- `pe[pos, 2*i+1] = cos( pos * ω_i )`\n",
    "- where `ω_i = 10000 ** ( -2*i / d_model )`\n",
    "\n",
    "**Intuition:** Even columns use sine, odd columns use cosine, with frequencies that shrink as `i` grows. This gives a smooth, multi-frequency code for positions without learning extra parameters.\n",
    "\n",
    "**Example:** Let `d_model = 8` (so 4 sine/cosine pairs) and `pos = 5`. At this position, we have a position embedding vector. \n",
    "Frequencies use ω_i = 10000^(-2i / d_model):\n",
    "\n",
    "- ω_0 = 10000^{-0/8} = 1\n",
    "- ω_1 = 10000^{-2/8} = 10000^{-0.25} = 0.1\n",
    "- ω_2 = 10000^{-4/8} = 10000^{-0.5} = 0.01\n",
    "- ω_3 = 10000^{-6/8} = 10000^{-0.75} = 0.001\n",
    "\n",
    "Therefore, the postiion vector is \n",
    "\n",
    "[\n",
    "    sin(5 * 1), cos(5 * 1), sin(5 * 0.1), cos(5 * 0.1), sin(5 * 0.01), cos(5 * 0.01),  sin(5 * 0.001), cos(5 * 0.001)\n",
    "]\n",
    "\n",
    ", which is\n",
    "\n",
    "[\n",
    "-0.958924, 0.283662,\n",
    "0.479426, 0.877583,\n",
    "0.049979, 0.998750,\n",
    "0.005000, 0.999988\n",
    "]\n",
    "\n",
    "\n",
    "### PyTorch tips:\n",
    "\n",
    "Slicing notation you’ll use\n",
    "- `pe[:, 0::2]` → all rows, **even** columns (0, 2, 4, …)  \n",
    "- `pe[:, 1::2]` → all rows, **odd** columns (1, 3, 5, …)\n",
    "\n",
    "This lets you fill all even columns with `sin(...)` and all odd columns with `cos(...)` in one shot.\n",
    "\n",
    "`torch.arange(start, end, step)` gives you a sequence of position indices."
   ]
  },
  {
   "cell_type": "code",
   "id": "d72cae8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:58:06.781067Z",
     "start_time": "2025-11-07T00:58:06.759718Z"
    }
   },
   "source": [
    "# ===== TODO (Student) =====\n",
    "# Implement sinusoidal_positional_encoding(max_len, d_model)\n",
    "# Return a tensor of shape [max_len, d_model] (float32).\n",
    "# pe[pos, 0::2] = sin(pos / 10000^(2i/d_model))\n",
    "# pe[pos, 1::2] = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "def sinusoidal_positional_encoding(max_len: int, d_model: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Build sinusoidal positional encodings (no batch).\n",
    "    Args:\n",
    "        max_len: maximum sequence length\n",
    "        d_model: model hidden size (must be >=1)\n",
    "    Returns:\n",
    "        pe: [max_len, d_model] float tensor\n",
    "    Example:\n",
    "        >>> pe = sinusoidal_positional_encoding(3, 4)\n",
    "        >>> pe.shape\n",
    "        torch.Size([3, 4])\n",
    "        >>> torch.allclose(pe[0, 1::2], torch.zeros(2))  # sin(0)=0\n",
    "        True\n",
    "    \"\"\"\n",
    "    pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
    "    # TODO: Implement sinusoidal positional encoding\n",
    "    # - Use torch.arange to create position indices\n",
    "    # - Compute the div_term as in the formula\n",
    "    # - Apply the sin/cos functions to the position indices\n",
    "    # - Fill the even and odd elements with the computed values\n",
    "    # - Return the positional encoding tensor\n",
    "    tensor1 = torch.arange(0,max_len-1,2)\n",
    "    tensor2 = torch.arange(1,max_len-1,2)\n",
    "\n",
    "    pe = torch.arange(0, max_len-1,2)\n",
    "\n",
    "    print(tensor1)\n",
    "    print(tensor2)\n",
    "    for i in range(max_len):\n",
    "\n",
    "        ##Compute the div_term as in the formula\n",
    "        ω_i = 10000 ** ( -2 * i / d_model )\n",
    "        ## what is positon\n",
    "        pe[:, 0::2] = math.sin(pos / 10000^( 2* i/d_model))\n",
    "        pe[:, 1::2] = math.cos(pos / 10000^(2 * i/d_model))\n",
    "\n",
    "\n",
    "    return pe"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a4d75a9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:58:13.814673Z",
     "start_time": "2025-11-07T00:58:13.724718Z"
    }
   },
   "source": [
    "# Quick, deterministic tests\n",
    "_pe = sinusoidal_positional_encoding(16, 8)\n",
    "assert _pe.shape == (16, 8), \"Shape must be [L, D]\"\n",
    "# sin(0)=0 on even dims; cos(0)=1 on odd dims\n",
    "assert torch.allclose(_pe[0, 0::2], torch.zeros(4)), \"sin(0) should be 0 on even dims\"\n",
    "assert torch.allclose(_pe[0, 1::2], torch.ones(4)), \"cos(0) should be 1 on odd dims\"\n",
    "# Smoothness: position 1 should be closer to 2 than to 8\n",
    "d12 = torch.dist(_pe[1], _pe[2])\n",
    "d18 = torch.dist(_pe[1], _pe[8])\n",
    "assert d12 < d18, \"Nearby positions should be more similar than far-away\"\n",
    "print(\"✅ Task A tests passed\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  2,  4,  6,  8, 10, 12, 14])\n",
      "tensor([ 1,  3,  5,  7,  9, 11, 13])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "cos(0) should be 1 on odd dims",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAssertionError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 6\u001B[39m\n\u001B[32m      4\u001B[39m \u001B[38;5;66;03m# sin(0)=0 on even dims; cos(0)=1 on odd dims\u001B[39;00m\n\u001B[32m      5\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m torch.allclose(_pe[\u001B[32m0\u001B[39m, \u001B[32m0\u001B[39m::\u001B[32m2\u001B[39m], torch.zeros(\u001B[32m4\u001B[39m)), \u001B[33m\"\u001B[39m\u001B[33msin(0) should be 0 on even dims\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m torch.allclose(_pe[\u001B[32m0\u001B[39m, \u001B[32m1\u001B[39m::\u001B[32m2\u001B[39m], torch.ones(\u001B[32m4\u001B[39m)), \u001B[33m\"\u001B[39m\u001B[33mcos(0) should be 1 on odd dims\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Smoothness: position 1 should be closer to 2 than to 8\u001B[39;00m\n\u001B[32m      8\u001B[39m d12 = torch.dist(_pe[\u001B[32m1\u001B[39m], _pe[\u001B[32m2\u001B[39m])\n",
      "\u001B[31mAssertionError\u001B[39m: cos(0) should be 1 on odd dims"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85fe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, cfg: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.tok = nn.Embedding(cfg.vocab_size, cfg.d_model)\n",
    "        self.cfg = cfg\n",
    "        if cfg.pe_type == \"learned\":\n",
    "            self.pos = nn.Embedding(cfg.max_seq_len, cfg.d_model)\n",
    "        elif cfg.pe_type == \"sinusoidal\":\n",
    "            pe = sinusoidal_positional_encoding(cfg.max_seq_len, cfg.d_model)  # student fn\n",
    "            self.register_buffer(\"pe_table\", pe)  # [L,D]\n",
    "        else:\n",
    "            self.pos = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [B, S] token ids\n",
    "        returns: [B, S, D]\n",
    "        \"\"\"\n",
    "        B, S = x.shape\n",
    "        tok = self.tok(x)\n",
    "        if self.cfg.pe_type == \"none\":\n",
    "            return tok\n",
    "        elif self.cfg.pe_type == \"learned\":\n",
    "            pos_ids = torch.arange(S, device=x.device).unsqueeze(0)  # [1,S]\n",
    "            return tok + self.pos(pos_ids)\n",
    "        else:  # sinusoidal\n",
    "            return tok + self.pe_table[:S].unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a054a6",
   "metadata": {},
   "source": [
    "## Task B — Implement `scaled_dot_product_attention(q, k, v, mask=None)`\n",
    "\n",
    "**Notations**\n",
    "- **B** = batch size  \n",
    "- **H** = number of heads  \n",
    "- **S** = sequence length (number of tokens per example)  \n",
    "- **D_head** = head dimension (usually `D_model / H`)\n",
    "\n",
    "Per head, we work with:\n",
    "- **q, k, v**: `[B, H, S, D_head]` They are the Query, Key, Value matrices we are trying to learn\n",
    "- **scores**: `[B, H, S, S]`: It is the per-head **similarity matrix** between every query position *i* and every key position *j*. To get the individual score:\n",
    "    - Take the **query** vector for token `i`: `q_vec = q[b, h, i, :]`  (length = `D_head`)\n",
    "    - Take the **key** vector for token `j`: `k_vec = k[b, h, j, :]`   (length = `D_head`)\n",
    "    - Compute their **dot product**, then **scale** by `1 / sqrt(D_head)`: scores[b,h,i,j] = (q_vec . k_vec) / sqrt(D_head)\n",
    "    - In tensor form: scores = ( q @ k.transpose(-2, -1) ) / sqrt(D_head)\n",
    "\n",
    "- **attn_probs**: `[B, H, S, S]` It is the attention weight that represents how much token i should \"look at\" token j when building its new representation. From the attention scores we just computed:\n",
    "    - (Optional) apply a mask: set forbidden entries to a large negative value (e.g., `-1e9`) so they become ~0 after softmax.\n",
    "    - Apply **softmax over the last dim** (keys `j`) to turn scores into probabilities (so that the token i dimension forms a probability distribution over all possible tokens)\n",
    "\n",
    "- **output**: `[B, H, S, D_head]` It is the new representation. For example out[b, h, i, :] is the vector representation of token i after attending to all tokens (including itself). \n",
    "    - `attn_probs[b,h,i,j]` is how much token `i` attends to token `j`.  \n",
    "    - `v[b,h,j,:]` is the value vector at position `j`. \n",
    "    - The output is the weighted average of values across all `j`: out[b, h, i, :] = sum(attn_probs[b, h, i, j] * v[b, h, j, :] for j=0,1,...,S)\n",
    "\n",
    "**What to write.**\n",
    "1) Compute `scores = (q @ kᵀ) / √dₖ` → shape `[B, H, S, S]`.  \n",
    "2) If a boolean `mask` is provided (`True = keep`, `False = block`), set masked positions to a large negative number (e.g., `-1e9`) **before** softmax.  \n",
    "3) Softmax over the last dimension → `[B, H, S, S]`.  \n",
    "4) Multiply by `v` to get outputs → `[B, H, S, D]`. Return both `(out, attn_weights)`.\n",
    "\n",
    "### PyTorch Tips:\n",
    "`tensor.masked_fill(mask, value)` takes a boolean tensor (same size as the tensor) fills elements of the tensor with value where mask is True. \n",
    "\n",
    "To assign a large negative number for the block elements (when mask is False), you can do `scores = scores.masked_fill(~mask, -1e9)`\n",
    "\n",
    "**After you code:** run the tiny deterministic test. Look for **“✅ Tiny SDPA test passed”**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c582bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TODO (Student) =====\n",
    "# Implement scaled_dot_product_attention with an optional boolean mask.\n",
    "# Tensor shapes (per head):\n",
    "#   q, k, v: [B, H, S, D_head]  -- we compute attention over the last two dims\n",
    "#   mask:   [B, 1, S, S]    -- True means \"keep\", False means \"mask out\"\n",
    "# Return:\n",
    "#   out:  [B, H, S, D_head] \n",
    "#   attn: [B, H, S, S]  (softmax weights)\n",
    "\n",
    "def scaled_dot_product_attention(\n",
    "    q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    >>> B,H,S,D = 1,1,2,2\n",
    "    >>> q = torch.tensor([[[[10., 0.],[0., 10.]]]])  # sharp self-similarity\n",
    "    >>> k = q.clone()\n",
    "    >>> v = torch.tensor([[[[1., 2.],[3., 4.]]]])\n",
    "    >>> out, attn = scaled_dot_product_attention(q, k, v, None)\n",
    "    >>> attn.shape, out.shape\n",
    "    (torch.Size([1, 1, 2, 2]), torch.Size([1, 1, 2, 2]))\n",
    "    \"\"\"\n",
    "     # q,k,v: [B,H,S,D]\n",
    "    d_head = q.size(-1)\n",
    "    scores = ...  #TODO: compute attention scores from q, k, size should be [B,H,S,S]\n",
    "    if mask is not None:\n",
    "        # mask: True = keep, False = mask out\n",
    "        # TODO: replace the forbidden positions with a large negative number (-1e9)\n",
    "\n",
    "        pass\n",
    "    \n",
    "    scores = scores - scores.amax(dim=-1, keepdim=True) # to make scores numerically stable\n",
    "    attn = torch.softmax(scores, dim=-1)  # obtain the attention weights, shape [B,H,S,S]\n",
    "    out = torch.matmul(attn, v)           # obtain the new representation, shape [B,H,S,D]\n",
    "    return out, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d60714",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiny_test_sdpa():\n",
    "    # Shapes: B=1, H=1, S=2, D=1  → super small and inspectable\n",
    "    B,H,S,D = 1,1,2,1\n",
    "\n",
    "    # Choose q,k = zeros so QK^T/√d_k = [[0,0],[0,0]]\n",
    "    # -> softmax rows become [0.5, 0.5] without a mask (by symmetry).\n",
    "    q = torch.zeros(B, H, S, D, dtype=torch.float32)\n",
    "    k = torch.zeros(B, H, S, D, dtype=torch.float32)\n",
    "\n",
    "    # Pick easy values for v so the weighted sums are trivial:\n",
    "    # token0 value = 2, token1 value = 0\n",
    "    v = torch.tensor([[[[2.0], [0.0]]]], dtype=torch.float32)  # [1,1,2,1]\n",
    "\n",
    "    # --- Case 1: No mask ---\n",
    "    out, attn = scaled_dot_product_attention(q, k, v, mask=None)\n",
    "\n",
    "    # Expected: every row = [0.5, 0.5] (since scores are all zeros)\n",
    "    exp_attn = torch.tensor([[[[0.5, 0.5],\n",
    "                               [0.5, 0.5]]]], dtype=torch.float32)\n",
    "\n",
    "    # Output = attn @ v = 0.5*2 + 0.5*0 = 1 for each row\n",
    "    exp_out  = torch.tensor([[[[1.0],\n",
    "                               [1.0]]]], dtype=torch.float32)\n",
    "\n",
    "    assert attn.shape == (B,H,S,S) and out.shape == (B,H,S,D)\n",
    "    assert torch.allclose(attn, exp_attn, atol=1e-7), f\"Unmasked attn mismatch:\\n{attn}\\n!=\\n{exp_attn}\"\n",
    "    assert torch.allclose(out,  exp_out,  atol=1e-7), f\"Unmasked out mismatch:\\n{out}\\n!=\\n{exp_out}\"\n",
    "\n",
    "    # --- Case 2: With causal mask (True=keep, False=mask) ---\n",
    "    # Row 0 may only attend to token 0; Row 1 may attend to {0,1}.\n",
    "    mask = torch.tensor([[[[ True, False],\n",
    "                           [ True,  True]]]], dtype=torch.bool)  # [1,1,2,2]\n",
    "\n",
    "    out_m, attn_m = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "\n",
    "    # Expected attention:\n",
    "    #   row0: [1, 0]  (only token 0 allowed)\n",
    "    #   row1: [0.5, 0.5]  (both allowed, equal scores)\n",
    "    exp_attn_m = torch.tensor([[[[1.0, 0.0],\n",
    "                                 [0.5, 0.5]]]], dtype=torch.float32)\n",
    "    # Expected outputs:\n",
    "    #   row0: 1*2 + 0*0 = 2\n",
    "    #   row1: 0.5*2 + 0.5*0 = 1\n",
    "    exp_out_m  = torch.tensor([[[[2.0],\n",
    "                                 [1.0]]]], dtype=torch.float32)\n",
    "\n",
    "    assert attn_m.shape == (B,H,S,S) and out_m.shape == (B,H,S,D)\n",
    "    assert torch.allclose(attn_m, exp_attn_m, atol=1e-7), f\"Masked attn mismatch:\\n{attn_m}\\n!=\\n{exp_attn_m}\"\n",
    "    assert torch.allclose(out_m,  exp_out_m,  atol=1e-7), f\"Masked out mismatch:\\n{out_m}\\n!=\\n{exp_out_m}\"\n",
    "\n",
    "    print(\"✅ Tiny SDPA test passed\")\n",
    "\n",
    "# Run it:\n",
    "tiny_test_sdpa()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957aef7",
   "metadata": {},
   "source": [
    "## Task C — Split/Combine Heads \n",
    "\n",
    "### Why do we split into **multiple heads**?\n",
    "**Multi-Head Self-Attention (MHA)** computes **several attention maps in parallel**. Each “head”:\n",
    "- looks at the sequence through a **different learned projection** (its own k,q,v matrices),\n",
    "- produces its **own attention distribution** (its own softmax over positions),\n",
    "- returns a **head-specific context vector**.\n",
    "\n",
    "This lets the model **attend to different things at the same time** (e.g., one head tracks next-token agreement, another tracks long-range rhymes). With a **single head**, every token has only **one** attention distribution, so it must compromise between competing patterns. With MHA, each head computes its **own** scaled dot-product attention and softmax. Finally we **concatenate** all head outputs and mix them with a single output projection.\n",
    "\n",
    "\n",
    "To do that, we need two shape transforms:\n",
    "- `split_heads`: **[B, S, D_model] → [B, H, S, D_head]** (prepare per-head tensors)\n",
    "- `combine_heads`: **[B, H, S, D_head] → [B, S, D_model]** (stitch heads back together)\n",
    "\n",
    "This is *only* a rearrangement of the same numbers—no math, just shape moves—so attention can run independently per head.\n",
    "\n",
    "### 2) PyTorch ops you’ll use (and why)\n",
    "- **`view`**: change tensor dimensions **without** changing data.  \n",
    "  - `view` is fast but **requires contiguous memory**; it may error if the tensor is not contiguous.\n",
    "- **`transpose(dim0, dim1)`**: swap **two** dimensions (no data copy; just a view).  \n",
    "  For MHA we want to go from `[B, S, H, D_head]` to `[B, H, S, D_head]` (swap axes 1 and 2) we can use `transpose(1, 2)`\n",
    "- **`contiguous()`**: In PyTorch, A tensor is contiguous if its elements are stored in one uninterrupted block of memory. Some operations require the input tensor to be contiguous (e.g., view()). tensor.contiguous() returns a contiguous copy so that a subsequent `view(...)` is valid.  \n",
    "  After `transpose`, the tensor is often **not** contiguous; call `.contiguous()` before `view`.\n",
    "\n",
    "\n",
    "### 3) Shape recipes (step-by-step)\n",
    "\n",
    "- `split_heads(x: [B, S, D_model]) -> [B, H, S, D_head]`\n",
    "1. Check that `D_model % H == 0`, and let `D_head = D_model // H`.\n",
    "2. First, reshape the last dim into `(H, D_head)`:  \n",
    "   `x = x.view(B, S, H, D_head)` \n",
    "3. Bring `H` in front of `S` so heads sit right after batch:  \n",
    "   `x = x.transpose(1, 2)` → shape `[B, H, S, D_head]`\n",
    "4. Ensure contiguity: `x = x.contiguous()` (safe to add here).\n",
    "\n",
    "- `combine_heads(x: [B, H, S, D_head]) -> [B, S, D_model]`\n",
    "1. Swap back the `H` and `S` dims:  \n",
    "   `x = x.transpose(1, 2)` → `[B, S, H, D_head]`\n",
    "2. Make it contiguous (required if you’ll `view`):  \n",
    "   `x = x.contiguous()`\n",
    "3. Flatten the last two dims:  \n",
    "   `x = x.view(B, S, H * D_head)` (or `reshape`) → `[B, S, D_model]`\n",
    "\n",
    "**After you code:** run the test — you should see **“✅ Task C tests passed”**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1becb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TODO (Student) =====\n",
    "# Implement split_heads and combine_heads round-trip.\n",
    "\n",
    "def split_heads(x: torch.Tensor, num_heads: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: [B, S, D_model] -> [B, H, S, D_head]\n",
    "    \"\"\"\n",
    "    # TODO: Implement split_heads\n",
    "    # - Check that D_model % H == 0\n",
    "    # - Let D_head = D_model // H\n",
    "    # - Reshape the last dim into (H, D_head)\n",
    "    # - Bring H in front of S\n",
    "    # - Ensure contiguity\n",
    "    # - Return the transposed tensor\n",
    "    pass\n",
    "\n",
    "def combine_heads(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x: [B, H, S, D_head] -> [B, S, D_model]\n",
    "    \"\"\"\n",
    "    # TODO: Implement combine_heads\n",
    "    # - Swap back the H and S dims\n",
    "    # - Make it contiguous\n",
    "    # - Flatten the last two dims\n",
    "    # - Return the reshaped tensor\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8845d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B,S,D_model,H = 2,4,8,2\n",
    "x = torch.arange(B*S*D_model, dtype=torch.float32).reshape(B,S,D_model)\n",
    "xh = split_heads(x, H)\n",
    "xc = combine_heads(xh)\n",
    "assert xh.shape == (B,H,S,D_model//H), \"split_heads wrong shape\"\n",
    "assert xc.shape == (B,S,D_model), \"combine_heads wrong shape\"\n",
    "assert torch.allclose(x, xc), \"combine(split(x)) must equal x\"\n",
    "print(\"✅ Task C tests passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78343730",
   "metadata": {},
   "source": [
    "## Task D — Implement `MultiHeadSelfAttention.forward(x, mask)`\n",
    "\n",
    "**Steps.**\n",
    "1) Linear projections: `Q = Wq x`, `K = Wk x`, `V = Wv x`.  \n",
    "2) `split_heads` for each.  \n",
    "3) Call your `scaled_dot_product_attention(Q, K, V, mask)`.  \n",
    "4) `combine_heads` and project with `Wo`. Return `(y, attn_map)`.\n",
    "\n",
    "**Sanity check.** With `n_heads=1` and all projection matrices set to identity, MHA must match the single-head SDPA on the same input (we test this for you). Expect **“✅ Task D tests passed”**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3215580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== TODO (Student) =====\n",
    "# Implement the forward pass of MHA:\n",
    "# 1) Project X -> Q,K,V\n",
    "# 2) split to heads\n",
    "# 3) scaled_dot_product_attention (with mask)\n",
    "# 4) combine heads -> output projection\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_model, self.n_heads = d_model, n_heads\n",
    "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        x: [B,S,D], mask: [B,1,S,S] or None\n",
    "        returns: y: [B,S,D], attn: [B,H,S,S]\n",
    "        \"\"\"\n",
    "        B,S,D = x.shape\n",
    "        # TODO: Implement the forward pass of MHA:\n",
    "        # 1) Project X -> Q,K,V\n",
    "        # 2) split to heads\n",
    "        # 3) scaled_dot_product_attention (with mask)\n",
    "        # 4) combine heads and output projection\n",
    "        # 5) return output and attention weights\n",
    "        pass\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c26d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity test with 1 head and identity projections → attention reduces to single-head SDPA\n",
    "B,S,D = 1,3,4\n",
    "x = torch.randn(B,S,D)\n",
    "mha = MultiHeadSelfAttention(d_model=D, n_heads=1)\n",
    "\n",
    "# Set projections to identity\n",
    "with torch.no_grad():\n",
    "    mha.q_proj.weight.copy_(torch.eye(D))\n",
    "    mha.k_proj.weight.copy_(torch.eye(D))\n",
    "    mha.v_proj.weight.copy_(torch.eye(D))\n",
    "    mha.o_proj.weight.copy_(torch.eye(D))\n",
    "\n",
    "mask = make_causal_mask(S)\n",
    "y, attn = mha(x, mask=mask)\n",
    "assert y.shape == (B,S,D) and attn.shape == (B,1,S,S)\n",
    "# Compare with manual SDPA on the same x\n",
    "q = x.unsqueeze(1)  # [B,1,S,D]\n",
    "k = x.unsqueeze(1)\n",
    "v = x.unsqueeze(1)\n",
    "manual, manual_attn = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "assert torch.allclose(y, manual.squeeze(1), atol=1e-5), \"MHA(1 head) must match SDPA\"\n",
    "print(\"✅ Task D tests passed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b06b0",
   "metadata": {},
   "source": [
    "## Quick Run / Inspect\n",
    "\n",
    "We have provided the building blocks for our transformer LM. You can just run the following code and see the logits, loss, and attention maps.\n",
    "\n",
    "We keep defaults very small so it runs in minutes on CPU:\n",
    "- Model: `d_model=128`, `n_heads=4`, `n_layers=2`, GELU FFN, no dropout.\n",
    "- Batch/bptt sizes are tiny in the toy trainer.\n",
    "\n",
    "You can train for a few hundred steps. Then try the `generate(...)` helper:\n",
    "- **Temperature** < 1.0 → safer/peakier choices; > 1.0 → more diverse.\n",
    "- **Top-k** keeps only the k most likely tokens.\n",
    "- **Top-p** (nucleus) keeps the smallest set whose cumulative probability ≥ p.\n",
    "\n",
    "If you skip training, you can still run `generate(...)` on the untrained model to see random-ish outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f00afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, ff_mult: int = 4, p_drop: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadSelfAttention(d_model, n_heads)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, ff_mult * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(ff_mult * d_model, d_model),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        attn_out, attn = self.mha(self.ln1(x), mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        ff_out = self.ff(self.ln2(x))\n",
    "        x = x + self.dropout(ff_out)\n",
    "        return x, attn\n",
    "\n",
    "class TinyTransformerLM(nn.Module):\n",
    "    def __init__(self, cfg: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = TokenAndPositionEmbedding(cfg)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(cfg.d_model, cfg.n_heads, cfg.ff_mult) for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(cfg.d_model)\n",
    "        self.lm_head = nn.Linear(cfg.d_model, cfg.vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, targets: Optional[torch.Tensor] = None):\n",
    "        # x: [B,S]\n",
    "        B,S = x.shape\n",
    "        h = self.embed(x)  # [B,S,D]\n",
    "        mask = make_causal_mask(S).to(x.device)\n",
    "        attn_maps = []\n",
    "        for blk in self.blocks:\n",
    "            h, attn = blk(h, mask)\n",
    "            attn_maps.append(attn)  # list of [B,H,S,S]\n",
    "        h = self.ln_f(h)\n",
    "        logits = self.lm_head(h)  # [B,S,V]\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss, attn_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcf215",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ModelConfig(d_model=128, n_heads=4, n_layers=2, max_seq_len=128, pe_type=\"sinusoidal\")\n",
    "model = TinyTransformerLM(cfg).to(DEVICE)\n",
    "\n",
    "# One tiny batch\n",
    "x, y = batched_random_chunks(train_ids, block_size=64, batch_size=2)\n",
    "logits, loss, attn_maps = model(x, y)\n",
    "print(\"logits:\", logits.shape, \"| loss:\", float(loss))\n",
    "print(\"attn maps:\", [a.shape for a in attn_maps])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee07df",
   "metadata": {},
   "source": [
    "## Task E — Train & Sample from the Tiny Transformer LM\n",
    "\n",
    "**What you do:** We provide the training loop. Just run the training cell, then use the `generate(...)` helper to sample text. The training time should be within 5 minutes.\n",
    "\n",
    "Expected outcomes:\n",
    "- Val loss curve: decreasing quickly at first, then flattening. If val loss rises while train falls, stop earlier. The training and validation losses should be below ~1.9.\n",
    "- Sampled texts: the samples should have \"Shakespeare-ish structure\" (uppercase speaker tags, dialogues, line breaks) and plausible (fake) words. It's normal that the sentences are not semantically consistent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tiny(model: nn.Module, steps=300, batch_size=64, block_size=128, lr=5e-3):\n",
    "    model.train()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95))\n",
    "    for t in range(steps):\n",
    "        x, y = batched_random_chunks(train_ids, block_size, batch_size)\n",
    "        logits, loss, _ = model(x, y)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if (t+1) % 50 == 0:\n",
    "            with torch.no_grad():\n",
    "                vx, vy = batched_random_chunks(val_ids, block_size, batch_size)\n",
    "                _, vloss, _ = model(vx, vy)\n",
    "            print(f\"step {t+1:4d} | train {loss.item():.3f} | val {vloss.item():.3f}\")\n",
    "    return model\n",
    "\n",
    "# run it (comment out to skip in CPU-only tight time)\n",
    "model = train_tiny(model, steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8046dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, prompt: str, max_new_tokens=200, temperature=1.0, top_k=None, top_p=None):\n",
    "    model.eval()\n",
    "    ids = encode(prompt).unsqueeze(0).to(DEVICE)  # [1,S]\n",
    "    for _ in range(max_new_tokens):\n",
    "        ids_cond = ids[:, -model.cfg.max_seq_len:]\n",
    "        logits, _, _ = model(ids_cond, None)\n",
    "        logits = logits[:, -1, :] / max(1e-6, temperature)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(probs, k=top_k, dim=-1)\n",
    "            thresh = v[:, -1].unsqueeze(-1)\n",
    "            probs = torch.where(probs >= thresh, probs, torch.zeros_like(probs))\n",
    "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        if top_p is not None:\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "            cum = torch.cumsum(sorted_probs, dim=-1)\n",
    "            keep = cum <= top_p\n",
    "            keep[..., 0] = True\n",
    "            filtered = torch.where(keep, sorted_probs, torch.zeros_like(sorted_probs))\n",
    "            filtered = filtered / filtered.sum(dim=-1, keepdim=True)\n",
    "            # sample from filtered\n",
    "            next_id = torch.multinomial(filtered, num_samples=1)\n",
    "            next_token = sorted_idx.gather(-1, next_id)\n",
    "        else:\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        ids = torch.cat([ids, next_token], dim=1)\n",
    "    return decode(ids[0].cpu())\n",
    "\n",
    "# Try different knobs after (optionally) training:\n",
    "print(generate(model, \"ROMEO:\\n\", max_new_tokens=200, temperature=0.8, top_k=50))\n",
    "print(\"--------------------------------\")\n",
    "print(generate(model, \"JULIET:\\n\", max_new_tokens=200, temperature=1.0, top_p=0.9))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09aeba7",
   "metadata": {},
   "source": [
    "## Mini Reflection\n",
    "\n",
    "- Switch `pe_type` among `\"none\"` and `\"sinusoidal\"`. After a short run, which produced the best validation loss and most coherent samples?\n",
    "- Try a couple of `temperature` and `top_k`/`top_p` settings in `generate`. What happens to diversity vs repetition?\n",
    "\n",
    "Write 2–4 bullet points on what you observe (1-2 sentences each)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bbf405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write down your observations here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
